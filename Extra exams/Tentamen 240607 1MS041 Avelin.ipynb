{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec2656c8",
   "metadata": {},
   "source": [
    "1.6 Exam vB, PROBLEM 1\n",
    "Maximum Points = 14\n",
    "In this problem you will do rejection sampling from complicated distributions, you will also be using\n",
    "your samples to compute certain integrals, a method known as Monte Carlo integration: (Keep in\n",
    "mind that choosing a good sampling distribution is often key to avoid too much rejection)\n",
    "1. [4p] Fill in the remaining part of the function problem1_rejection in order to produce samples\n",
    "from the below density using rejection sampling:\n",
    "$$f[x] = Cx^{0.2}(1 − x)^{1.3}$$\n",
    "for 0 ≤ x ≤ 1, where C is a value such that f above is a density (i.e. integrates to one). Hint: you\n",
    "do not need to know the value of C to perform rejection sampling.\n",
    "\n",
    "2. [2p] Produce 100000 samples (use fewer if it takes too long) and put the answer in\n",
    "problem1_samples from the above distribution and plot the histogram.\n",
    "\n",
    "3. [2p] Define X as a random variable with the density given in part 1. Denote Y = sin(10X)\n",
    "and use the above 100000 samples to estimate$$E[Y]$$and store the result in problem1_expectation.\n",
    "\n",
    "4. [2p] Use Hoeffdings inequality to produce a 95% confidence interval of the expectation above\n",
    "and store the result as a tuple in the variable problem1_interval\n",
    "\n",
    "5. [4p] Can you calculate an approximation of the value of C from part 1 using random samples?\n",
    "Provide a plot of the histogram from part 2 together with the true density as a curve, recall\n",
    "that this requires the value of C. Explain what method you used and what answer you got."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47161de2",
   "metadata": {},
   "source": [
    "# Problem 1: Rejection Sampling and Monte Carlo Integration\n",
    "\n",
    "## Part 1: Rejection Sampling\n",
    "\n",
    "The target density is:\n",
    "\n",
    "\\[\n",
    "f(x) = C \\, x^{0.2} (1 - x)^{1.3}, \\quad 0 \\le x \\le 1\n",
    "\\]\n",
    "\n",
    "We **don’t need C** for rejection sampling. A natural choice is to use a **Uniform(0,1)** proposal distribution because the support is [0,1].\n",
    "\n",
    "The **rejection sampling algorithm** is:\n",
    "\n",
    "1. Pick a proposal \\(x \\sim U(0,1)\\)  \n",
    "2. Pick \\(u \\sim U(0,1)\\)  \n",
    "3. Accept \\(x\\) if \\(u \\le f(x)/M\\), where \\(M\\) is a constant such that \\(f(x)/M \\le 1\\)\n",
    "\n",
    "Since \\(f(x) = x^{0.2} (1-x)^{1.3}\\) (ignoring C) and the maximum occurs at\n",
    "\n",
    "\\[\n",
    "x = \\frac{0.2}{0.2+1.3} \\approx 0.1333\n",
    "\\]\n",
    "\n",
    "we can compute the maximum value:\n",
    "\n",
    "\\[\n",
    "f_{\\max} = x^{0.2} (1-x)^{1.3} \\big|_{x=0.1333} \\approx 0.903\n",
    "\\]\n",
    "\n",
    "So we can safely use \\(M=1\\) (or slightly above the max).\n",
    "\n",
    "## Part 4: 95% Confidence Interval Using Hoeffding's Inequality\n",
    "\n",
    "Hoeffding’s inequality states that for independent random variables \\(Y_i \\in [a, b]\\):\n",
    "\n",
    "\\[\n",
    "P\\Big(|\\bar{Y} - \\mathbb{E}[Y]| \\ge t\\Big) \\le 2 \\exp\\Big(- \\frac{2 n t^2}{(b-a)^2}\\Big)\n",
    "\\]\n",
    "\n",
    "- Here \\(a = -1\\), \\(b = 1\\) since \\(\\sin(10X) \\in [-1, 1]\\).  \n",
    "- Solve for \\(t\\) such that \\(2 \\exp(- 2 n t^2 / (b-a)^2) = 0.05\\):\n",
    "\n",
    "\\[\n",
    "t = \\sqrt{\\frac{(b-a)^2 \\ln(2/0.05)}{2 n}} \n",
    "= \\sqrt{\\frac{4 \\ln(40)}{2 \\cdot 100000}} \\approx 0.0094\n",
    "\\]\n",
    "\n",
    "## Part 5: Approximation of \\(C\\) and Plot True Density\n",
    "\n",
    "To approximate the normalizing constant \\(C\\), recall that for a probability density function:\n",
    "\n",
    "\\[\n",
    "\\int_0^1 f(x) \\, dx = 1 \\quad \\implies \\quad C = \\frac{1}{\\int_0^1 x^{0.2} (1-x)^{1.3} \\, dx}\n",
    "\\]\n",
    "\n",
    "We can estimate the integral using **Monte Carlo integration** by drawing uniform samples \\(u_i \\sim \\text{Uniform}(0,1)\\) and computing the mean of \\(u_i^{0.2} (1-u_i)^{1.3}\\):\n",
    "\n",
    "\n",
    "### Explanation (Part 5.2.1)\n",
    "\n",
    "We used **Monte Carlo integration** to estimate the normalizing constant \\(C\\). Since:\n",
    "\n",
    "\\[\n",
    "C = \\frac{1}{\\int_0^1 x^{0.2} (1-x)^{1.3} \\, dx},\n",
    "\\]\n",
    "\n",
    "we drew uniform samples \\(u_i \\sim \\text{Uniform}(0,1)\\) and approximated the integral as:\n",
    "\n",
    "\\[\n",
    "\\int_0^1 x^{0.2} (1-x)^{1.3} \\, dx \\approx \\frac{1}{N} \\sum_{i=1}^{N} u_i^{0.2} (1-u_i)^{1.3}.\n",
    "\\]\n",
    "\n",
    "Then \\(C\\) is simply the reciprocal of this estimate.  \n",
    "\n",
    "This method produces a value of \\(C\\) that, when used in the true density, matches the histogram of our rejection-sampled data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42037f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1\n",
    "#def problem1_rejection(n_samples=1):\n",
    "# Distribution from part 1\n",
    "# write the code in this function to produce samples from the distribution\n",
    "# in the assignment\n",
    "# Return a numpy array of length n_samples\n",
    "#    return XXX\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def problem1_rejection(n_samples=1):\n",
    "    samples = []\n",
    "    while len(samples) < n_samples:\n",
    "        x = np.random.uniform(0, 1)\n",
    "        u = np.random.uniform(0, 1)\n",
    "        fx = x**0.2 * (1-x)**1.3\n",
    "        if u <= fx:  # Rejection criterion\n",
    "            samples.append(x)\n",
    "    return np.array(samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393cbf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate 100000 samples\n",
    "problem1_samples = problem1_rejection(100000)\n",
    "\n",
    "# Plot histogram\n",
    "plt.hist(problem1_samples, bins=50, density=True, alpha=0.6, color='skyblue')\n",
    "plt.title(\"Histogram of Samples from f(x)\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992c0b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3\n",
    "X = problem1_samples\n",
    "Y = np.sin(10 * X)\n",
    "problem1_expectation = np.mean(Y)\n",
    "problem1_expectation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c885ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 4\n",
    "n = len(Y)\n",
    "a, b = -1, 1\n",
    "delta = 0.05\n",
    "t = np.sqrt((b-a)**2 * np.log(2/delta) / (2*n))\n",
    "problem1_interval = (problem1_expectation - t, problem1_expectation + t)\n",
    "problem1_interval\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31a58c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 5\n",
    "# Estimate the integral of x^0.2 * (1-x)^1.3\n",
    "uniform_samples = np.random.uniform(0, 1, 100000)\n",
    "integral_estimate = np.mean(uniform_samples**0.2 * (1 - uniform_samples)**1.3)\n",
    "problem1_C = 1 / integral_estimate\n",
    "problem1_C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7df842d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 5\n",
    "# Write your code to produce the plot here\n",
    "#XXXXXXX\n",
    "\n",
    "x_vals = np.linspace(0, 1, 200)\n",
    "f_true = problem1_C * x_vals**0.2 * (1 - x_vals)**1.3\n",
    "\n",
    "plt.hist(problem1_samples, bins=50, density=True, alpha=0.6, color='skyblue', label='Samples')\n",
    "plt.plot(x_vals, f_true, color='red', lw=2, label='True Density')\n",
    "plt.title(\"Histogram and True Density\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73561098",
   "metadata": {},
   "source": [
    "2 Part 5\n",
    "Double click this cell and directly edit below to answer part 5\n",
    "\n",
    "2.0.1 Begin explanation\n",
    "\n",
    "Bla di bla\n",
    "\n",
    "2.0.2 End explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdf0d72",
   "metadata": {},
   "source": [
    "2.1 Exam vB, PROBLEM 2\n",
    "Maximum Points = 13\n",
    "Let us build a proportional model (P(Y = 1 | X) = G(β0 + β · X) where G is the logistic function)\n",
    "for the spam vs not spam data. Here we assume that the features are presence vs not presence of a\n",
    "word, let X1, X2, X3 denote the presence (1) or absence (0) of the words (”free”, ”prize”, ”win”).\n",
    "1. [2p] Load the file data/spam.csv and create two numpy arrays, problem2_X which has\n",
    "shape (n_emails,3) where each feature in problem2_X corresponds to X1, X2, X3 from above,\n",
    "problem2_Y which has shape (n_emails,) and consists of a 1 if the email is spam and 0 if\n",
    "it is not. Split this data into a train-calibration-test sets where we have the split 40%, 20%,\n",
    "40%, put this data in the designated variables in the code cell.\n",
    "2. [4p] Follow the calculation from the lecture notes where we derive the logistic regression and\n",
    "implement the final loss function inside the class ProportionalSpam. You can use the Test\n",
    "cell to check that it gives the correct value for a test-point.\n",
    "3. [4p] Train the model problem2_ps on the training data. The goal is to calibrate the probabilities output from the model. Start by creating a new variable problem2_X_pred (shape\n",
    "(n_samples,1)) which consists of the predictions of problem2_ps on the calibration dataset.\n",
    "Then train a calibration model using sklearn.tree.DecisionTreeRegressor, store this\n",
    "trained model in problem2_calibrator.\n",
    "4. [3p] Use the trained model problem2_ps and the calibrator problem2_calibrator to make\n",
    "final predictions on the testing data, store the prediction in problem2_final_predictions.\n",
    "Compute the 0 − 1 test-loss and store it in problem2_01_loss and provide a 99% confidence\n",
    "interval of it, store this in the variable problem2_interval, this should again be a tuple as in\n",
    "problem1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f42ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1\n",
    "problem2_X = XXX\n",
    "problem2_Y = XXX\n",
    "problem2_X_train = XXX\n",
    "problem2_X_calib = XXX\n",
    "problem2_X_test = XXX\n",
    "problem2_Y_train = XXX\n",
    "problem2_Y_calib = XXX\n",
    "problem2_Y_test = XXX\n",
    "print(problem2_X_train.shape,problem2_X_calib.shape,\n",
    "problem2_X_test.shape,problem2_Y_train.shape,problem2_Y_calib.shape,problem2_Y_test.shape)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(\"data/spam.csv\")\n",
    "\n",
    "# Features\n",
    "problem2_X = df[['free', 'prize', 'win']].values\n",
    "problem2_Y = df['spam'].values  # assuming 'spam' column is 1 for spam, 0 otherwise\n",
    "\n",
    "# First split: 60% train+calib, 40% test\n",
    "X_temp, problem2_X_test, Y_temp, problem2_Y_test = train_test_split(problem2_X, problem2_Y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Second split: split temp into 40% train, 20% calib\n",
    "train_ratio = 0.4 / 0.6  # 0.4 of total is train\n",
    "problem2_X_train, problem2_X_calib, problem2_Y_train, problem2_Y_calib = train_test_split(X_temp, Y_temp, test_size=1-train_ratio, random_state=42)\n",
    "\n",
    "print(problem2_X_train.shape, problem2_X_calib.shape, problem2_X_test.shape)\n",
    "print(problem2_Y_train.shape, problem2_Y_calib.shape, problem2_Y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade847c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2\n",
    "class ProportionalSpam(object):\n",
    "    def __init__(self):\n",
    "        self.coeffs = None\n",
    "        self.result = None\n",
    "\n",
    "    # Logistic loss\n",
    "    def loss(self, X, Y, coeffs):\n",
    "        beta0 = coeffs[0]\n",
    "        beta = coeffs[1:]\n",
    "        eta = beta0 + X.dot(beta)\n",
    "        G = np.exp(eta) / (1 + np.exp(eta))\n",
    "        # To avoid log(0) numerical errors\n",
    "        G = np.clip(G, 1e-10, 1 - 1e-10)\n",
    "        return -np.sum(Y * np.log(G) + (1 - Y) * np.log(1 - G))\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        from scipy import optimize\n",
    "        opt_loss = lambda coeffs: self.loss(X, Y, coeffs)\n",
    "        initial_arguments = np.zeros(X.shape[1] + 1)\n",
    "        self.result = optimize.minimize(opt_loss, initial_arguments, method='cg')\n",
    "        self.coeffs = self.result.x \n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.coeffs is None:\n",
    "            raise ValueError(\"Model not trained\")\n",
    "        beta0 = self.coeffs[0]\n",
    "        beta = self.coeffs[1:]\n",
    "        eta = beta0 + X.dot(beta)\n",
    "        G = np.exp(eta) / (1 + np.exp(eta))\n",
    "        # Round to 0.1 for calibration purposes\n",
    "        return np.round(10 * G) / 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da2f622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Train logistic regression\n",
    "problem2_ps = ProportionalSpam()\n",
    "problem2_ps.fit(problem2_X_train, problem2_Y_train)\n",
    "\n",
    "# Predictions on calibration set\n",
    "problem2_X_pred = problem2_ps.predict(problem2_X_calib).reshape(-1, 1)\n",
    "\n",
    "# Train Decision Tree Regressor as calibrator\n",
    "problem2_calibrator = DecisionTreeRegressor(max_depth=3, random_state=42)\n",
    "problem2_calibrator.fit(problem2_X_pred, problem2_Y_calib)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0919ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 4\n",
    "# These are the predicted probabilities\n",
    "problem2_final_predictions = XXX\n",
    "# In order to compute this loss we first need to convert the predicted␣\n",
    ",→probabilities to a decision\n",
    "# recall the Bayes classifier?\n",
    "problem2_01_loss = XXX\n",
    "# Recall the interval is given as a tuple (a,b) or a list [a,b]\n",
    "problem2_interval = XXX\n",
    "\n",
    "# Logistic regression predictions on test set\n",
    "X_test_pred_raw = problem2_ps.predict(problem2_X_test).reshape(-1, 1)\n",
    "\n",
    "# Calibrated predictions\n",
    "problem2_final_predictions = problem2_calibrator.predict(X_test_pred_raw)\n",
    "\n",
    "# Convert probabilities to 0/1 decisions\n",
    "y_pred_class = (problem2_final_predictions >= 0.5).astype(int)\n",
    "\n",
    "# 0-1 loss\n",
    "problem2_01_loss = np.mean(y_pred_class != problem2_Y_test)\n",
    "\n",
    "# 99% confidence interval for 0-1 loss using Hoeffding inequality\n",
    "n_test = len(problem2_Y_test)\n",
    "delta = 0.01\n",
    "t = np.sqrt(np.log(2/delta) / (2 * n_test))\n",
    "problem2_interval = (problem2_01_loss - t, problem2_01_loss + t)\n",
    "\n",
    "problem2_01_loss, problem2_interval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094778f9",
   "metadata": {},
   "source": [
    "Markov chain A: A to A: 0.8, A to B: 0.2, B to A: 0.6, B to B: 0.2, B to C: 0.2, C to B: 0.4, C to D: 0.6, D to C: 0.8, D to D: 0.2\n",
    "\n",
    "Markov chain B: A to D: 0.8, D to C: 0.5, C to B: 1, B to A: 0, A to B: 0.2, B to C: 1, C to D 0, D to A: 0.5\n",
    "\n",
    "Markov chain C: A to A: 0.2, A to B: 0.3, B to A: 0.2, B to B: 0.2, B to C: 0.6, C to B: 0.4, C to C: 0, C to D: 0.6, D to C: 0, D to D: 0.6, D to E: 0.4, E to D: 0.4, E to E: 0.6, E to A: 0, A to E: 0.5 \n",
    " \n",
    "Markov chain D: A to A: 0.8, A to B: 0.2, B to A: 0.6, B to B: 0.2, B to C: 0.2, C to B: 0.4, C to D: 0.6, D to C: 0.7, D to D: 0.2, D to A: 0.1\n",
    "\n",
    "1. [2p] What is the transition matrix?\n",
    "2. [2p] Is the Markov chain irreducible?\n",
    "3. [3p] Is the Markov chain aperiodic? What is the period for each state? Hint: Recall our\n",
    "definition of period; Let $T := \\{t ∈ N : P_t(x, x) > 0\\}$ and the greatest common divisor of T is\n",
    "the period.\n",
    "4. [3p] Does the Markov chain have a stationary distribution, and if so, what is it?\n",
    "5. [3p] Is the Markov chain reversible?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9575934e",
   "metadata": {},
   "source": [
    "Markov Chain A\n",
    "\n",
    "Given:\n",
    "\n",
    "A → A: 0.8, A → B: 0.2\n",
    "\n",
    "B → A: 0.6, B → B: 0.2, B → C: 0.2\n",
    "\n",
    "C → B: 0.4, C → D: 0.6\n",
    "\n",
    "D → C: 0.8, D → D: 0.2\n",
    "\n",
    " Markov Chain B\n",
    "\n",
    "Given:\n",
    "\n",
    "A → D: 0.8, A → B: 0.2\n",
    "\n",
    "B → A: 0, B → C: 1\n",
    "\n",
    "C → B: 1, C → D: 0\n",
    "\n",
    "D → A: 0.5, D → C: 0.5\n",
    "\n",
    "Markov Chain C\n",
    "\n",
    "States: A, B, C, D, E\n",
    "\n",
    "Given:\n",
    "\n",
    "A → A: 0.2, A → B: 0.3, A → E: 0.5\n",
    "\n",
    "B → A: 0.2, B → B: 0.2, B → C: 0.6\n",
    "\n",
    "C → B: 0.4, C → C: 0, C → D: 0.6\n",
    "\n",
    "D → C: 0, D → D: 0.6, D → E: 0.4\n",
    "\n",
    "E → D: 0.4, E → E: 0.6\n",
    "\n",
    "Markov Chain D\n",
    "\n",
    "Given:\n",
    "\n",
    "A → A: 0.8, A → B: 0.2\n",
    "\n",
    "B → A: 0.6, B → B: 0.2, B → C: 0.2\n",
    "\n",
    "C → B: 0.4, C → D: 0.6\n",
    "\n",
    "D → C: 0.7, D → D: 0.2, D → A: 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064c21dc",
   "metadata": {},
   "outputs": [],
   "source": [
    " # PART 1\n",
    "#------------------------TRANSITION MATRIX -------------------------------\n",
    "# Answer each one by supplying the transition matrix as a numpy array\n",
    "# of shape (n_states,n_states), where state (A,B,...) corresponds to index (0,1,...)\n",
    "import numpy as np\n",
    "\n",
    "problem3_A = np.array([\n",
    "    [0.8, 0.2, 0.0, 0.0],  # A\n",
    "    [0.6, 0.2, 0.2, 0.0],  # B\n",
    "    [0.0, 0.4, 0.0, 0.6],  # C\n",
    "    [0.0, 0.0, 0.8, 0.2]   # D\n",
    "])\n",
    "\n",
    "problem3_B = np.array([\n",
    "    [0.0, 0.2, 0.0, 0.8],  # A\n",
    "    [0.0, 0.0, 1.0, 0.0],  # B\n",
    "    [0.0, 1.0, 0.0, 0.0],  # C\n",
    "    [0.5, 0.0, 0.5, 0.0]   # D\n",
    "])\n",
    "problem3_C = np.array([\n",
    "    [0.2, 0.3, 0.0, 0.0, 0.5],  # A\n",
    "    [0.2, 0.2, 0.6, 0.0, 0.0],  # B\n",
    "    [0.0, 0.4, 0.0, 0.6, 0.0],  # C\n",
    "    [0.0, 0.0, 0.0, 0.6, 0.4],  # D\n",
    "    [0.0, 0.0, 0.0, 0.4, 0.6]   # E\n",
    "])\n",
    "problem3_D = np.array([\n",
    "    [0.8, 0.2, 0.0, 0.0],  # A\n",
    "    [0.6, 0.2, 0.2, 0.0],  # B\n",
    "    [0.0, 0.4, 0.0, 0.6],  # C\n",
    "    [0.1, 0.0, 0.7, 0.2]   # D\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522ad860",
   "metadata": {},
   "source": [
    "A Markov chain is irreducible if all states communicate.\n",
    "Looking at the graphs:\n",
    "\n",
    "A: all states communicate → True\n",
    "\n",
    "B: A→D→C→B, etc → True\n",
    "\n",
    "C: all 5 states communicate → True\n",
    "\n",
    "D: all 4 states communicate → True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fa3380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 2\n",
    "#------------------------REDUCIBLE -------------------------------\n",
    "# Answer each one with a True or False\n",
    "problem3_A_irreducible = True\n",
    "problem3_B_irreducible = True\n",
    "problem3_C_irreducible = True\n",
    "problem3_D_irreducible = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b6f36b",
   "metadata": {},
   "source": [
    "Period of a state x is gcd(t : P^t(x,x)>0), look for self-loops\n",
    "| Chain | Self-loops?                  | Aperiodic?                                  |\n",
    "| ----- | ---------------------------- | ------------------------------------------- |\n",
    "| A     | A,B,D have loops             | Yes, all states reachable with loops → True |\n",
    "| B     | Only some loops, but gcd = 1 | Yes → True                                  |\n",
    "| C     | All loops exist except C     | Check gcd of return times → 1 → True        |\n",
    "| D     | Loops exist → True           | True                                        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205902c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 3\n",
    "#------------------------APERIODIC-------------------------------\n",
    "# Answer each one with a True or False\n",
    "problem3_A_is_aperiodic = True\n",
    "problem3_B_is_aperiodic = True\n",
    "problem3_C_is_aperiodic = True\n",
    "problem3_D_is_aperiodic = True\n",
    "\n",
    "# Answer the following with the period of the states as a numpy array\n",
    "# of shape (n_states,)\n",
    "problem3_A_periods = np.array([1,1,1,1])\n",
    "problem3_B_periods = np.array([1,1,1,1])\n",
    "problem3_C_periods = np.array([1,1,1,1,1])\n",
    "problem3_D_periods = np.array([1,1,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1faf654",
   "metadata": {},
   "source": [
    "Solve pi*P = pi, Sum over i: pi_i = 1\n",
    "All finite, irreducible, aperiodic chains have a unique stationary distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7456bde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 4\n",
    "#------------------------STATIONARY DISTRIBUTION-----------------\n",
    "# Answer each one with a True or False\n",
    "problem3_A_has_stationary = True\n",
    "problem3_B_has_stationary = True\n",
    "problem3_C_has_stationary = True\n",
    "problem3_D_has_stationary = True\n",
    "# Answer the following with the stationary distribution as a numpy array of shape (n_states,)\n",
    "# if the Markov chain has a stationary distribution otherwise answer with False\n",
    "# Example: using numpy to solve linear system\n",
    "def stationary(P):\n",
    "    n = P.shape[0]\n",
    "    A = np.vstack([P.T - np.eye(n), np.ones(n)])\n",
    "    b = np.append(np.zeros(n), 1)\n",
    "    pi = np.linalg.lstsq(A, b, rcond=None)[0]\n",
    "    return pi\n",
    "\n",
    "problem3_A_stationary_dist = stationary(problem3_A)\n",
    "problem3_B_stationary_dist = stationary(problem3_B)\n",
    "problem3_C_stationary_dist = stationary(problem3_C)\n",
    "problem3_D_stationary_dist = stationary(problem3_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b8bfb0",
   "metadata": {},
   "source": [
    "A Markov chain is reversible if pi_i*P_ij = pi_j * P_ji for all i,j\n",
    "\n",
    "| Chain | Reversible?                                  |\n",
    "| ----- | -------------------------------------------- |\n",
    "| A     | Yes (probabilities satisfy detailed balance) |\n",
    "| B     | No (many transitions are 0 in one direction) |\n",
    "| C     | No (asymmetry exists)                        |\n",
    "| D     | No (asymmetry exists)                        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fee2ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 5\n",
    "#------------------------REVERSIBLE-----------------\n",
    "# Answer each one with a True or False\n",
    "problem3_A_is_reversible = True\n",
    "problem3_B_is_reversible = False\n",
    "problem3_C_is_reversible = False\n",
    "problem3_D_is_reversible = False\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
