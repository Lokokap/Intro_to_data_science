{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b65947f0",
   "metadata": {},
   "source": [
    "1.6 Exam vB, PROBLEM 1\n",
    "Maximum Points = 14\n",
    "In this problem you will do rejection sampling from complicated distributions, you will also be using\n",
    "your samples to compute certain integrals, a method known as Monte Carlo integration: (Keep in\n",
    "mind that choosing a good sampling distribution is often key to avoid too much rejection)\n",
    "1. [4p] Fill in the remaining part of the function problem1_rejection in order to produce samples\n",
    "from the below distribution using rejection sampling: (Hint: F is the distribution function)\n",
    "$$F[x] = 0, x â‰¤ 0,\\frac{e^{x^2} - x^2 - 1}{e -2}, 0 < x < 1, 1, x â‰¥ 1$$\n",
    "2. [2p] Produce 100000 samples (use fewer if it takes too long (more than 10 sec) and\n",
    "you cannot find a solution) and put the answer in problem1_samples from the above\n",
    "distribution and plot the histogram together with the true density.\n",
    "3. [2p] Use the above 100000 samples (problem1_samples) to approximately compute the integral\n",
    "$$\\int^1_0 sin(x) \\frac{2(e^{x^2}-1)x}{e - 2}dx$$\n",
    "and store the result in problem1_integral.\n",
    "4. [2p] Use Hoeffdings inequality to produce a 95% confidence interval of the integral above and\n",
    "store the result as a tuple in the variable problem1_interval\n",
    "5. [4p] Fill in the remaining part of the function problem1_rejection_2 in order to produce\n",
    "samples from the below distribution using rejection sampling:\n",
    "$$F[x] = 0, x â‰¤ 0, 20xe^{20âˆ’1/x}, 0 < x < 1/20 1, x â‰¥1/20$$\n",
    "Hint: this is tricky because if you choose the wrong sampling distribution you reject at least\n",
    "9 times out of 10. You will get points based on how long your code takes to create a certain\n",
    "number of samples, if you choose the correct sampling distribution you can easily create 100000\n",
    "samples within 2 seconds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d1e0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1\n",
    "# Distribution from part 1\n",
    "# write the code in this function to produce samples from the distribution\n",
    "# in the assignment\n",
    "# Make sure you choose a good sampling distribution to avoid unnecessary\n",
    "# rejections\n",
    "# Return a numpy array of length n_samples\n",
    "import numpy as np\n",
    "\n",
    "def problem1_rejection(n_samples=1):\n",
    "    samples = []\n",
    "    M = 2 * (np.e - 1) / (np.e - 2)  # safe upper bound\n",
    "\n",
    "    while len(samples) < n_samples:\n",
    "        x = np.random.rand()\n",
    "        u = np.random.rand()\n",
    "        fx = (2 * x * (np.exp(x**2) - 1)) / (np.e - 2)\n",
    "        if u <= fx / M:\n",
    "            samples.append(x)\n",
    "\n",
    "    return np.array(samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734c8635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "problem1_samples = problem1_rejection(100000)\n",
    "\n",
    "# True density\n",
    "x_grid = np.linspace(0, 1, 500)\n",
    "true_density = (2 * x_grid * (np.exp(x_grid**2) - 1)) / (np.e - 2)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(problem1_samples, bins=50, density=True, alpha=0.6)\n",
    "plt.plot(x_grid, true_density)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66c7eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3\n",
    "problem1_integral = np.mean(np.sin(problem1_samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d0e193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 4\n",
    "n = len(problem1_samples)\n",
    "epsilon = np.sqrt(np.log(2 / 0.05) / (2 * n))\n",
    "\n",
    "problem1_interval = (\n",
    "    problem1_integral - epsilon,\n",
    "    problem1_integral + epsilon\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233ac9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 5\n",
    "\n",
    "#Let ð‘Œ = 1/X, then Y has exponential like tail so we sample Y from Exp(1)\n",
    "\n",
    "\n",
    "# Distribution from part 2\n",
    "# write the code in this function to produce samples from the distributionâ£\n",
    "# in the assignment\n",
    "# Make sure you choose a good sampling distribution to avoid unnecessary\n",
    "# rejections\n",
    "    # Return a numpy array of length n_samples\n",
    "def problem1_rejection_2(n_samples=1):\n",
    "    samples = []\n",
    "\n",
    "    while len(samples) < n_samples:\n",
    "        y = np.random.exponential() + 20  # shift\n",
    "        x = 1 / y\n",
    "\n",
    "        if x < 1 / 20:\n",
    "            u = np.random.rand()\n",
    "            fx = 20 * np.exp(20 - 1/x) / x**2\n",
    "            gy = np.exp(-(y - 20))         # proposal density\n",
    "            M = 20 * np.exp(20)\n",
    "\n",
    "            if u <= fx / (M * gy):\n",
    "                samples.append(x)\n",
    "\n",
    "    return np.array(samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdda536",
   "metadata": {},
   "source": [
    "1.7 Exam vB, PROBLEM 2\n",
    "Maximum Points = 14\n",
    "In this problem we have data consisting of user behavior on a website. The pages of the website are\n",
    "just numbers in the dataset 0, 1, 2, . . . and each row consists of a user, a source and a destination\n",
    "page. This signifies that the user was on the source page and clicked a link leading them to the\n",
    "destination page. The goal is to improve the user experience by decreasing load time of the next\n",
    "page visited, as such we need a good estimate for the next site likely to be visited. We will model\n",
    "this using a homogeneous Markov chain, each row in the data-file then corresponds to a single\n",
    "realization of a transition.\n",
    "1. [3p] Load the data in the file data/websites.csv and construct a matrix of size n_pages x n_pages which is the maximum likelihood estimate of the true transition matrix for the\n",
    "Markov chain. Here the ordering of the states are exactly the ones in the data-file, that is\n",
    "page 0 has index 0 in the matrix.\n",
    "2. [4p] A page loads in Exp(3) (Exponentially distributed with mean 1/3) seconds if not preloaded\n",
    "and loads with Exp(20) (Exponentially distributed with mean 1/20) seconds if preloaded and\n",
    "we only preload the most likely next site. Given that we start in page 8 simulate 10000 load\n",
    "times from page 1 (that is, only a single step), store the result in the variable indicated in\n",
    "the cell. Repeat the experiment but this time preload the two most likely pages and store the\n",
    "result in the indicated variable.\n",
    "3. [3p] Compare the average (empirical) load time from part 2 with the theoretical one of no\n",
    "pre-loading. Does the load time improve, how did you come to this conclusion? (Explain in\n",
    "the free text field).\n",
    "4. [4p] Calculate the stationary distribution of the Markov chain and calculate the expected load\n",
    "time with respect to the stationary distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7784e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: 3 points\n",
    "# Load the data from the file data/websites.csv and estimate the transition\n",
    "# matrix of the Markov chain\n",
    "# Store the estimated transition matrix in the variable\n",
    "# problem2_transition_matrix below\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"data/websites.csv\")\n",
    "\n",
    "# Assume columns are named as follows (typical):\n",
    "# user_id, source, destination\n",
    "sources = df.iloc[:, 1].values\n",
    "destinations = df.iloc[:, 2].values\n",
    "\n",
    "# Number of states\n",
    "problem2_n_states = int(max(sources.max(), destinations.max())) + 1\n",
    "\n",
    "# Count transitions\n",
    "counts = np.zeros((problem2_n_states, problem2_n_states))\n",
    "\n",
    "for s, d in zip(sources, destinations):\n",
    "    counts[s, d] += 1\n",
    "\n",
    "# Normalize rows to get transition probabilities\n",
    "problem2_transition_matrix = counts / counts.sum(axis=1, keepdims=True)\n",
    " # A numpy array of shape (problem2_n_states,\n",
    "# problem2_n_states)\n",
    "# Store the number of states in the variable problem2_n_states below\n",
    "# An integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b069284f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_load_times(start_page, P, preload_pages, n=10000):\n",
    "    load_times = []\n",
    "\n",
    "    for _ in range(n):\n",
    "        next_page = np.random.choice(len(P), p=P[start_page])\n",
    "\n",
    "        if next_page in preload_pages:\n",
    "            load_times.append(np.random.exponential(scale=1/20))\n",
    "        else:\n",
    "            load_times.append(np.random.exponential(scale=1/3))\n",
    "\n",
    "    return np.array(load_times)\n",
    "\n",
    "\n",
    "# Part 2: 4 points\n",
    "# Simulate the website load times for the next page of 10000 users that are\n",
    "# currently on page 8 (recall indexing starts at 0) when we only preload the\n",
    "# most likely page.\n",
    "# Store the simulated page load times in the variable\n",
    "# problem2_page_load_times_top below\n",
    "start_page = 8\n",
    "row = problem2_transition_matrix[start_page]\n",
    "\n",
    "top_page = np.argmax(row)\n",
    "\n",
    "problem2_page_load_times_top = simulate_load_times(\n",
    "    start_page,\n",
    "    problem2_transition_matrix,\n",
    "    preload_pages={top_page},\n",
    "    n=10000\n",
    ")\n",
    " # A numpy array of shape (10000,)\n",
    "# Repeat the simulation of load times for the next page of 10000 users that are\n",
    "# currently on page 8 when we preload the two most likely pages.\n",
    "# Store the simulated page load times in the variable\n",
    "# problem2_page_load_times_two below\n",
    "top_two_pages = set(np.argsort(row)[-2:])\n",
    "\n",
    "problem2_page_load_times_two = simulate_load_times(\n",
    "    start_page,\n",
    "    problem2_transition_matrix,\n",
    "    preload_pages=top_two_pages,\n",
    "    n=10000\n",
    ")\n",
    " # A numpy array of shape (10000,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11215482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3: 3 points\n",
    "# Calculate the true expected load time for loading a page without pre-loading\n",
    "# the next page and store it in the variable below\n",
    "problem2_avg = 1/3\n",
    " # A float\n",
    "# Is the average load time for loading a page without pre-loading the next page\n",
    "# larger than the average load time for loading a page after pre-loading the\n",
    "# next most likely page?\n",
    "problem2_comparison = (\n",
    "    problem2_page_load_times_top.mean() < problem2_avg\n",
    ") # True / False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6252b13",
   "metadata": {},
   "source": [
    "1.8 Free text answer\n",
    "Put the explanation for part 3 of how you made the decision about problem2_comparison below\n",
    "this line in this cell. In order to enter edit mode you can doubleclick this cell or select it and press\n",
    "enter.\n",
    "\n",
    "The expected load time without preloading is 1/3 seconds since page loads follow an Exp(3) distribution. I compared this theoretical value with the empirical average load time obtained when preloading the most likely next page. Since the empirical average load time is smaller than 1/3, preloading improves the load time. This conclusion is based on comparing the expected values rather than individual samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b111a4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 4: 4 points\n",
    "# Begin by calculating the stationary distribution of the Markov chain and store it in the variable below\n",
    "# WARNING: Since the transition matrix is not symmetric, numpy might make the output of the eigenvectors complex, you can use np.real() to get the real part of the eigenvectors\n",
    "# Store the stationary distribution in the variable below called problem2_stationary_distribution\n",
    "# Eigenvector of P^T corresponding to eigenvalue 1\n",
    "eigvals, eigvecs = np.linalg.eig(problem2_transition_matrix.T)\n",
    "\n",
    "idx = np.argmin(np.abs(eigvals - 1))\n",
    "stationary = np.real(eigvecs[:, idx])\n",
    "stationary = stationary / stationary.sum()\n",
    "\n",
    "problem2_stationary_distribution = stationary\n",
    " # A numpy array of shape (problem2_n_states,)\n",
    "# Now use the above stationary distribution to calculate the average load time\n",
    "# for loading a page after pre-loading the next most likely page according to\n",
    "# the stationary distribution\n",
    "# Store the average load time in the variable below\n",
    "expected_load_times = []\n",
    "\n",
    "for i in range(problem2_n_states):\n",
    "    p_max = problem2_transition_matrix[i].max()\n",
    "    expected_time = p_max * (1/20) + (1 - p_max) * (1/3)\n",
    "    expected_load_times.append(expected_time)\n",
    "\n",
    "expected_load_times = np.array(expected_load_times)\n",
    "\n",
    "problem2_avg_stationary = np.dot(\n",
    "    problem2_stationary_distribution,\n",
    "    expected_load_times\n",
    ")\n",
    " # A float\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df67bddd",
   "metadata": {},
   "source": [
    "1.9 Exam vB, PROBLEM 3\n",
    "Maximum Points = 12\n",
    "In this problem we are interested in fraud detection in an e-commerce system. In this problem we\n",
    "are given the outputs of a classifier that predicts the probabilities of fraud, your goal is to explore\n",
    "the threshold choice as in individual assignment 4. The costs associated with the predictions are:\n",
    "\n",
    "â€¢ True Positive (TP): Detecting fraud and blocking the transaction costs the company 100\n",
    "(manual review etc.)\n",
    "\n",
    "â€¢ True Negative (TN): Allowing a legitimate transaction has no cost.\n",
    "\n",
    "â€¢ False Positive (FP): Incorrectly classifying a legitimate transaction as fraudulent costs 120\n",
    "(customer dissatisfaction plus operational expenses for reversing the decision).\n",
    "\n",
    "â€¢ False Negative (FN): Missing a fraudulent transaction costs the company 600 (e.g., fraud loss plus potential reputational damage or penalties).\n",
    "\n",
    "The code cells contain more detailed instructions, THE FIRST CODE CELL INITIALIZES YOUR VARIABLES\n",
    "\n",
    "1. [3p] Complete filling the function cost to compute the average cost of a prediction model\n",
    "under a certain prediction threshold. Plot the cost as a function of the threshold (using the\n",
    "validation data provided in the first code cell of this problem), between 0 and 1 with 0.01\n",
    "increments.\n",
    "2. [2.5p] Find the threshold that minimizes the cost and calculate the cost at that threshold on\n",
    "the validation data. Also calculate the precision and recall at the optimal threshold on the\n",
    "validation data on class 1 and 0.\n",
    "3. [2.5p] Repeat step 2, but this time find the best threshold to minimize the 0âˆ’1 loss. Calculate\n",
    "the difference in cost between the threshold found in part 2 with the one just found in part 3.\n",
    "4. [4p] Provide a confidence interval around the optimal cost (with 95% confidence) applied to\n",
    "the test data and explain all the assumption you made.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4668b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL TO GET THE DATA\n",
    "# We start by loading the data\n",
    "import pandas as pd\n",
    "PROBLEM3_DF = pd.read_csv('data/fraud.csv')\n",
    "Y = PROBLEM3_DF['Class'].values\n",
    "X = PROBLEM3_DF[['V%d' % i for i in range(1,5)]+['Amount']].values\n",
    "# We will split the data into training, testing and validation sets\n",
    "from Utils import train_test_validation\n",
    "PROBLEM3_X_train, PROBLEM3_X_test, PROBLEM3_X_val, PROBLEM3_y_train,â£\n",
    ",â†’PROBLEM3_y_test, PROBLEM3_y_val =â£\n",
    ",â†’train_test_validation(X,Y,shuffle=True,random_state=1)\n",
    "# From this we will train a logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(PROBLEM3_X_train,PROBLEM3_y_train)\n",
    "# THE FOLLOWING CODE WILL PRODUCE THE ARRAYS YOU NEED FOR THE PROBLEM\n",
    "PROBLEM3_y_pred_proba_val = lr.predict_proba(PROBLEM3_X_val)[:,1]\n",
    "PROBLEM3_y_true_val = PROBLEM3_y_val\n",
    "PROBLEM3_y_pred_proba_test = lr.predict_proba(PROBLEM3_X_test)[:,1]\n",
    "PROBLEM3_y_true_test = PROBLEM3_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96300c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: 3 points\n",
    "# Implement the following function that calculates the cost of a binary\n",
    "# classifier according to\n",
    "# the specification in the problem statement\n",
    "# See the comments inside the function for details of the parameters\n",
    "#def cost(y_true,y_predict_proba,threshold):\n",
    "# y_true is a numpy array of shape (n_samples,) with binary labels\n",
    "# y_predict_proba is a numpy array of shape (n_samples,) with predicted\n",
    "# probabilities\n",
    "# threshold is a float between 0 and 1\n",
    "# When returning the cost, you should return the average cost per sample\n",
    "# thus it should be a value\n",
    "#    return XXX # A float\n",
    "# Provide the code below to plot the cost as a function of the threshold\n",
    "# using the validation data, specifically the arrays PROBLEM3_y_true_val and\n",
    "# PROBLEM3_y_pred_proba_val.\n",
    "# The plot should be between 0 and 1 with 0.01 increments\n",
    "# The y-axis should be the cost and the x-axis should be the threshold\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "def cost(y_true, y_predict_proba, threshold):\n",
    "    \"\"\"\n",
    "    Compute average cost per sample for a classifier under a given threshold.\n",
    "\n",
    "    TP cost = 100\n",
    "    TN cost = 0\n",
    "    FP cost = 120\n",
    "    FN cost = 600\n",
    "    \"\"\"\n",
    "    y_pred = (y_predict_proba >= threshold).astype(int)\n",
    "    \n",
    "    # True positives, false positives, true negatives, false negatives\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    TN = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    \n",
    "    total_cost = TP*100 + TN*0 + FP*120 + FN*600\n",
    "    avg_cost = total_cost / len(y_true)\n",
    "    \n",
    "    return avg_cost\n",
    "\n",
    "thresholds = np.arange(0, 1.01, 0.01)\n",
    "costs = [cost(PROBLEM3_y_true_val, PROBLEM3_y_pred_proba_val, t) for t in thresholds]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(thresholds, costs)\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Average cost per sample\")\n",
    "plt.title(\"Cost vs Threshold on validation set\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead71f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: 2.5 points\n",
    "# Use the cost function you just implemented above to find the threshold that minimizes the cost\n",
    "# using the validation data, specifically the arrays PROBLEM3_y_true_val and PROBLEM3_y_pred_proba_val.\n",
    "# Store the threshold in the variable below\n",
    "problem3_threshold = XXX # A float between 0 and 1\n",
    "# Now calculate the cost of the classifier using the validation data and the\n",
    "# threshold you just found\n",
    "# using the validation data, specifically the arrays PROBLEM3_y_true_val and\n",
    "# PROBLEM3_y_pred_proba_val.\n",
    "# Store the cost in the variable below\n",
    "problem3_cost_val = XXX # A float\n",
    "# Using the threshold you just found, calculate the predicted labels of the\n",
    "# classifier on the validation data\n",
    "# put the predicted labels in the variable below\n",
    "problem3_y_pred_val = XXX # A numpy array of shape (n_samples,) with values 0 or 1\n",
    "# Calculate the precision and recall of the classifier of class 1 using the\n",
    "# threshold you just found\n",
    "# using the validation data, specifically the arrays PROBLEM3_y_true_val and\n",
    "# PROBLEM3_y_pred_proba_val.\n",
    "problem3_precision_1 = XXX # A float between 0 and 1\n",
    "problem3_recall_1 = XXX # A float between 0 and 1\n",
    "# Calculate the precision and recall of the classifier of class 0 using the\n",
    "# threshold you just found\n",
    "# using the validation data, specifically the arrays PROBLEM3_y_true_val and\n",
    "# PROBLEM3_y_pred_proba_val.\n",
    "problem3_precision_0 = XXX # A float between 0 and 1\n",
    "problem3_recall_0 = XXX # A float between 0 and 1\n",
    "\n",
    "# Optimal threshold\n",
    "problem3_threshold = thresholds[np.argmin(costs)]\n",
    "\n",
    "# Cost at optimal threshold\n",
    "problem3_cost_val = cost(PROBLEM3_y_true_val, PROBLEM3_y_pred_proba_val, problem3_threshold)\n",
    "\n",
    "# Predicted labels at optimal threshold\n",
    "problem3_y_pred_val = (PROBLEM3_y_pred_proba_val >= problem3_threshold).astype(int)\n",
    "\n",
    "# Precision and recall for class 1\n",
    "problem3_precision_1 = precision_score(PROBLEM3_y_true_val, problem3_y_pred_val, pos_label=1)\n",
    "problem3_recall_1 = recall_score(PROBLEM3_y_true_val, problem3_y_pred_val, pos_label=1)\n",
    "\n",
    "# Precision and recall for class 0\n",
    "problem3_precision_0 = precision_score(PROBLEM3_y_true_val, problem3_y_pred_val, pos_label=0)\n",
    "problem3_recall_0 = recall_score(PROBLEM3_y_true_val, problem3_y_pred_val, pos_label=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935c6ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Part 3: 2.5 points\n",
    "# Find the threshold that minimizes the $0-1$ loss using the validation data\n",
    "# specifically the arrays PROBLEM3_y_true_val and PROBLEM3_y_pred_proba_val.\n",
    "# Store the threshold in the variable below\n",
    "problem3_threshold_01 = XXX # A float between 0 and 1\n",
    "# Now calculate the difference in cost (using the cost function you implemented\n",
    "# in step 1) between the optimal one chosen in part 2 and the one chosen in part 3\n",
    "# by taking the cost with the threshold found in part 3 and subtracting the\n",
    "# cost with the threshold found in part 2 to get a positive value\n",
    "problem3_cost_difference = XXX # A float\n",
    "\n",
    "# 0-1 loss = classification error\n",
    "errors = [np.mean((PROBLEM3_y_true_val != (PROBLEM3_y_pred_proba_val >= t).astype(int))) for t in thresholds]\n",
    "problem3_threshold_01 = thresholds[np.argmin(errors)]\n",
    "\n",
    "# Difference in cost between optimal threshold and 0-1 loss threshold\n",
    "problem3_cost_difference = abs(\n",
    "    cost(PROBLEM3_y_true_val, PROBLEM3_y_pred_proba_val, problem3_threshold_01)\n",
    "    - problem3_cost_val\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f587f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 4: 4 points\n",
    "# Using the threshold problem3_threshold use Hoeffdings inequality to provide a\n",
    "# confidence interval\n",
    "# for the cost of the classifier with 95 % confidence using the test data.\n",
    "# Specifically the arrays PROBLEM3_y_true_test and PROBLEM3_y_pred_proba_test.\n",
    "# Store the lower and upper bounds of the confidence interval in the variables\n",
    "# below\n",
    "n_test = len(PROBLEM3_y_true_test)\n",
    "C_hat = cost(PROBLEM3_y_true_test, PROBLEM3_y_pred_proba_test, problem3_threshold)\n",
    "\n",
    "epsilon = np.sqrt(np.log(2/0.05) / (2*n_test))\n",
    "\n",
    "problem3_lower_bound = C_hat - epsilon\n",
    "problem3_upper_bound = C_hat + epsilon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bcf3a8",
   "metadata": {},
   "source": [
    "1.10 Free text answer\n",
    "Put your explanation for part 4 below this line in this cell. Doubleclick to enter edit mode as before.\n",
    "\n",
    "I assumed that the individual sample costs are independent and bounded between 0 and the maximum cost (here 600 for a false negative). Using Hoeffdingâ€™s inequality, I constructed a 95% confidence interval around the empirical mean cost on the test set. This gives a conservative, distribution-free bound on the true expected cost per transaction. The interval represents where we expect the true average cost to lie with 95% confidence, assuming the test samples are independent realizations from the same distribution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
